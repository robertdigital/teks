{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"tEKS \u00b6 tEKS is a set of Terraform / Terragrunt modules designed to get you everything you need to run a production EKS cluster on AWS. It ships with sensible defaults, and add a lot of common addons with their configurations that work out of the box. the v5 and further version of this project have been completely revamp and now offer a skeleton to use as a base for your infrastructure projects around EKS. All the modules have been moved outside this repository and get their own versioning. The old README is accessible here Main purposes \u00b6 The main goal of this project is to glue together commonly used tooling with Kubernetes/EKS and to get from an AWS Account to a production cluster with everything you need without any manual configuration. What you get \u00b6 A production cluster all defined in IaaC with Terraform/Terragrunt: AWS VPC if needed based on terraform-aws-vpc EKS cluster base on terraform-aws-eks Kubernetes addons based on terraform-kubernetes-addons : provides various addons that are often used on Kubernetes and specifically on EKS. Kubernetes namespaces quota management based on terraform-kubernetes-namespaces : allows administrator to manage namespaces and quotas from a centralized configuration with Terraform. AWS ECR registries management based on terraform-aws-ecr Everything is tied together with Terragrunt and allows you to deploy a multi cluster architecture in a matter of minutes (ok maybe an hour) and different AWS accounts for different environments. Curated Features \u00b6 The main additionals features are the curated addons list, see here and in the customization of the cluster policy Enforced security \u00b6 Default PSP is removed and sensible defaults are enforced All addons have specific PSP enabled No IAM credentials on instances, everything is enforced with IRSA or KIAM Each addons is deployed in it's own namespace with sensible default network policies Out of the box monitoring \u00b6 Prometheus Operator with defaults dashboards Addons that support metrics are enable along with their serviceMonitor Custom grafana dashboard are available by default. Helm v3 provider \u00b6 All addons support Helm v3 configuration All charts are easily customizable Other and not limited to \u00b6 priorityClasses for addons use of [ kubectl-provider ], no more local exec and custom manifest are properly handled lot of manual stuff have been automated under the hood Requirements \u00b6 Terragrunt is not a hard requirement but all the modules are tested with Terragrunt. Terraform Terragrunt kubectl helm aws-iam-authenticator Examples \u00b6 terraform/live folder provides an opinionated directory structure for a production environment with an example using Additional infrastructure blocks \u00b6 If you wish to extend your infrastructure you can pick up additional modules on the clusterfrak-dynamics github page Branches \u00b6 master : Backward incompatible with v1.X but compatible with v2.X, releases bumped to v3.X because a lot has changed. release-1.X : Compatible with Terraform < 0.12 and Terragrunt < 0.19. Be sure to target the same modules version. release-2.X : Compatible with Terraform >= 0.12 and Terragrunt >= 0.19. Be sure to target the same modules version. License \u00b6","title":"Overview"},{"location":"#teks","text":"tEKS is a set of Terraform / Terragrunt modules designed to get you everything you need to run a production EKS cluster on AWS. It ships with sensible defaults, and add a lot of common addons with their configurations that work out of the box. the v5 and further version of this project have been completely revamp and now offer a skeleton to use as a base for your infrastructure projects around EKS. All the modules have been moved outside this repository and get their own versioning. The old README is accessible here","title":"tEKS"},{"location":"#main-purposes","text":"The main goal of this project is to glue together commonly used tooling with Kubernetes/EKS and to get from an AWS Account to a production cluster with everything you need without any manual configuration.","title":"Main purposes"},{"location":"#what-you-get","text":"A production cluster all defined in IaaC with Terraform/Terragrunt: AWS VPC if needed based on terraform-aws-vpc EKS cluster base on terraform-aws-eks Kubernetes addons based on terraform-kubernetes-addons : provides various addons that are often used on Kubernetes and specifically on EKS. Kubernetes namespaces quota management based on terraform-kubernetes-namespaces : allows administrator to manage namespaces and quotas from a centralized configuration with Terraform. AWS ECR registries management based on terraform-aws-ecr Everything is tied together with Terragrunt and allows you to deploy a multi cluster architecture in a matter of minutes (ok maybe an hour) and different AWS accounts for different environments.","title":"What you get"},{"location":"#curated-features","text":"The main additionals features are the curated addons list, see here and in the customization of the cluster policy","title":"Curated Features"},{"location":"#enforced-security","text":"Default PSP is removed and sensible defaults are enforced All addons have specific PSP enabled No IAM credentials on instances, everything is enforced with IRSA or KIAM Each addons is deployed in it's own namespace with sensible default network policies","title":"Enforced security"},{"location":"#out-of-the-box-monitoring","text":"Prometheus Operator with defaults dashboards Addons that support metrics are enable along with their serviceMonitor Custom grafana dashboard are available by default.","title":"Out of the box monitoring"},{"location":"#helm-v3-provider","text":"All addons support Helm v3 configuration All charts are easily customizable","title":"Helm v3 provider"},{"location":"#other-and-not-limited-to","text":"priorityClasses for addons use of [ kubectl-provider ], no more local exec and custom manifest are properly handled lot of manual stuff have been automated under the hood","title":"Other and not limited to"},{"location":"#requirements","text":"Terragrunt is not a hard requirement but all the modules are tested with Terragrunt. Terraform Terragrunt kubectl helm aws-iam-authenticator","title":"Requirements"},{"location":"#examples","text":"terraform/live folder provides an opinionated directory structure for a production environment with an example using","title":"Examples"},{"location":"#additional-infrastructure-blocks","text":"If you wish to extend your infrastructure you can pick up additional modules on the clusterfrak-dynamics github page","title":"Additional infrastructure blocks"},{"location":"#branches","text":"master : Backward incompatible with v1.X but compatible with v2.X, releases bumped to v3.X because a lot has changed. release-1.X : Compatible with Terraform < 0.12 and Terragrunt < 0.19. Be sure to target the same modules version. release-2.X : Compatible with Terraform >= 0.12 and Terragrunt >= 0.19. Be sure to target the same modules version.","title":"Branches"},{"location":"#license","text":"","title":"License"},{"location":"user-guides/ecr/","text":"ECR module \u00b6 ecr is a custom module maintained here and allows creation of AWS ECR repository to host docker images. It provides: ECR repository ECR repository policy Custom IAM user with Access Key and Secret Key to be able to push images to ECR (eg. for CI purposes) Customization \u00b6 Just like the other modules, custom configuration is done in terragrunt.hcl . include { path = \"${find_in_parent_folders()}\" } terraform { source = \"github.com/clusterfrak-dynamics/terraform-aws-ecr.git?ref=v2.1.0\" } locals { aws_region = yamldecode(file( \"${get_terragrunt_dir()}/${find_in_parent_folders(\" common_values.yaml \")}\" )) [ \"aws_region\" ] env = yamldecode(file( \"${get_terragrunt_dir()}/${find_in_parent_folders(\" common_tags.yaml \")}\" )) [ \"Env\" ] project = \"namespace\" prefix = yamldecode(file( \"${get_terragrunt_dir()}/${find_in_parent_folders(\" common_values.yaml \")}\" )) [ \"prefix\" ] custom_tags = yamldecode(file( \"${get_terragrunt_dir()}/${find_in_parent_folders(\" common_tags.yaml \")}\" )) } inputs = { env = local.env project = local.project prefix = local.prefix aws = { \"region\" = local.aws_region } custom_tags = merge( local.custom_tags ) registries = [ { name = \"${local.project}/myapp\" image_tag_mutability = \"MUTABLE\" scan_on_push = true }, ] registries_policies = [ ] }","title":"ECR"},{"location":"user-guides/ecr/#ecr-module","text":"ecr is a custom module maintained here and allows creation of AWS ECR repository to host docker images. It provides: ECR repository ECR repository policy Custom IAM user with Access Key and Secret Key to be able to push images to ECR (eg. for CI purposes)","title":"ECR module"},{"location":"user-guides/ecr/#customization","text":"Just like the other modules, custom configuration is done in terragrunt.hcl . include { path = \"${find_in_parent_folders()}\" } terraform { source = \"github.com/clusterfrak-dynamics/terraform-aws-ecr.git?ref=v2.1.0\" } locals { aws_region = yamldecode(file( \"${get_terragrunt_dir()}/${find_in_parent_folders(\" common_values.yaml \")}\" )) [ \"aws_region\" ] env = yamldecode(file( \"${get_terragrunt_dir()}/${find_in_parent_folders(\" common_tags.yaml \")}\" )) [ \"Env\" ] project = \"namespace\" prefix = yamldecode(file( \"${get_terragrunt_dir()}/${find_in_parent_folders(\" common_values.yaml \")}\" )) [ \"prefix\" ] custom_tags = yamldecode(file( \"${get_terragrunt_dir()}/${find_in_parent_folders(\" common_tags.yaml \")}\" )) } inputs = { env = local.env project = local.project prefix = local.prefix aws = { \"region\" = local.aws_region } custom_tags = merge( local.custom_tags ) registries = [ { name = \"${local.project}/myapp\" image_tag_mutability = \"MUTABLE\" scan_on_push = true }, ] registries_policies = [ ] }","title":"Customization"},{"location":"user-guides/eks-addons/","text":"EKS addons module \u00b6 eks-addons is a custom module maintained here and provides either: helm v3 charts manifests operators For commonly used addons one Kubernetes and most specifically with EKS. The deployments are curated to be tightly integrated with AWS and EKS. The following addons are available and work out of the box. Helm charts \u00b6 All charts have been tested with Helm v3 and the terraform-provider-helm v1.0 which supports Helm v3. They can be easily customize with custom values. cluster-autoscaler : scale worker nodes based on workload. external-dns : sync ingress and service records in route53. cert-manager : automatically generate TLS certificates, supports ACME v2. kiam : prevents pods to access EC2 metadata and enables pods to assume specific AWS IAM roles. nginx-ingress : processes Ingress object and acts as a HTTP/HTTPS proxy (compatible with cert-manager). metrics-server : enable metrics API and horizontal pod scaling (HPA). prometheus-operator : Monitoring / Alerting / Dashboards. fluentd-cloudwatch : forwards logs to AWS Cloudwatch. node-problem-detector : Forwards node problems to Kubernetes events flux : Continuous Delivery with Gitops workflow. sealed-secrets : Technology agnostic, store secrets on git. kong : API Gateway ingress controller. keycloak : Identity and access management Kubernetes Manifests \u00b6 Kubernetes manifests are deployed with terraform-provider-kubectl cni-metrics-helper : Provides cloudwatch metrics for VPC CNI plugins. Operator \u00b6 Some project are transitioning to Operators . Istio is going to drop Helm support and is not compatible with Helm v3 so it has been removed and replaced with the Istio operator istio-operator : Service mesh for Kubernetes. IAM permissions \u00b6 Some addons require specific IAM permission. This can be done by either: IRSA: IAM role for service account which is the default and recommended way Kiam Addons that need IAM access have two variables: create_resources_irsa : default to true and uses IAM role for service account create_resources_kiam : default to false and uses KIAM to manage IAM permissions There is no specific config, everything is taken care of by the module. Customization \u00b6 All the configuration is done in eks-addons/terragrunt.hcl . include { path = \"${find_in_parent_folders()}\" } terraform { source = \"github.com/clusterfrak-dynamics/terraform-kubernetes-addons.git?ref=v5.0.0\" before_hook \"init\" { commands = [ \"init\" ] execute = [ \"bash\" , \"-c\" , \"wget -O terraform-provider-kubectl https://github.com/gavinbunney/terraform-provider-kubectl/releases/download/v1.2.1/terraform-provider-kubectl-linux-amd64 && chmod +x terraform-provider-kubectl\" ] } } locals { env = yamldecode(file( \"${get_terragrunt_dir()}/${find_in_parent_folders(\" common_tags.yaml \")}\" )) [ \"Env\" ] aws_region = yamldecode(file( \"${get_terragrunt_dir()}/${find_in_parent_folders(\" common_values.yaml \")}\" )) [ \"aws_region\" ] } dependency \"eks\" { config_path = \"../eks\" mock_outputs = { cluster_id = \"cluster-name\" cluster_oidc_issuer_url = \"https://oidc.eks.eu-west-3.amazonaws.com/id/0000000000000000\" } } dependency \"vpc\" { config_path = \"../vpc\" mock_outputs = { private_subnets_cidr_blocks = [ \"10.0.0.0/16\" , \"192.168.0.0/24\" ] } } inputs = { cluster-name = dependency.eks.outputs.cluster_id aws = { \"region\" = local.aws_region } eks = { \"cluster_oidc_issuer_url\" = dependency.eks.outputs.cluster_oidc_issuer_url } nginx_ingress = { version = \"0.29.0\" chart_version = \"1.31.0\" enabled = true default_network_policy = true ingress_cidr = \"0.0.0.0/0\" use_nlb = false use_l 7 = false } istio_operator = { enabled = false } cluster_autoscaler = { create_iam_resources_kiam = false create_iam_resources_irsa = true iam_policy_override = \"\" version = \"v1.14.7\" chart_version = \"6.4.0\" enabled = true default_network_policy = true cluster_name = dependency.eks.outputs.cluster_id } external_dns = { create_iam_resources_kiam = false create_iam_resources_irsa = true iam_policy_override = \"\" version = \"0.6.0-debian-10-r0\" chart_version = \"2.18.0\" enabled = true default_network_policy = true } cert_manager = { create_iam_resources_kiam = false create_iam_resources_irsa = true iam_policy_override = \"\" version = \"v0.13.1\" chart_version = \"v0.13.1\" enabled = true default_network_policy = true acme_email = \"kevin@particule.io\" enable_default_cluster_issuers = true allowed_cidrs = dependency.vpc.outputs.private_subnets_cidr_blocks } kiam = { create_iam_user = true create_iam_resources = true assume_role_policy_override = \"\" version = \"v3.5\" chart_version = \"5.7.0\" enabled = false default_network_policy = false iam_user = \"\" } metrics_server = { version = \"v0.3.6\" chart_version = \"2.9.0\" enabled = true default_network_policy = true allowed_cidrs = dependency.vpc.outputs.private_subnets_cidr_blocks } flux = { create_iam_resources_kiam = false create_iam_resources_irsa = true version = \"1.18.0\" chart_version = \"1.2.0\" enabled = false default_network_policy = true extra_values = <<EXTRA_VALUES git : url : \"ssh://git@gitlab.com/myrepo/gitops-${local.env}.git\" pollInterval : \"2m\" rbac : create : false registry : automationInterval : \"2m\" EXTRA_VALUES } prometheus_operator = { chart_version = \"8.7.0\" enabled = true default_network_policy = true allowed_cidrs = dependency.vpc.outputs.private_subnets_cidr_blocks extra_values = <<EXTRA_VALUES grafana : deploymentStrategy : type : Recreate ingress : enabled : false annotations : kubernetes.io/ingress.class : nginx cert-manager.io/cluster-issuer : \"letsencrypt\" hosts : - grafana.clusterfrak-dynamics.io tls : - secretName : grafana-clusterfrak-dynamics-io hosts : - grafana.clusterfrak-dynamics.io persistence : enabled : true storageClassName : gp 2 accessModes : - ReadWriteOnce size : 10 Gi prometheus : prometheusSpec : replicas : 1 retention : 180 d ruleSelectorNilUsesHelmValues : false ruleNamespaceSelector : any : true serviceMonitorSelectorNilUsesHelmValues : false serviceMonitorNamespaceSelector : any : true storageSpec : volumeClaimTemplate : spec : storageClassName : gp 2 accessModes : [ \"ReadWriteOnce\" ] resources : requests : storage : 50 Gi EXTRA_VALUES } fluentd_cloudwatch = { create_iam_resources_kiam = false create_iam_resources_irsa = true default_network_policy = true iam_policy_override = \"\" chart_version = \"0.12.1\" version = \"v1.7.4-debian-cloudwatch-1.0\" enabled = true containers_log_retention_in_days = 180 } npd = { chart_version = \"1.6.3\" version = \"v0.8.0\" enabled = true default_network_policy = true } sealed_secrets = { chart_version = \"1.7.6\" version = \"v0.9.7\" enabled = true default_network_policy = true } cni_metrics_helper = { create_iam_resources_kiam = false create_iam_resources_irsa = true enabled = true version = \"v1.5.5\" iam_policy_override = \"\" } kong = { version = \"1.4\" chart_version = \"1.2.0\" enabled = false default_network_policy = true ingress_cidr = \"0.0.0.0/0\" } keycloak = { chart_version = \"7.0.0\" version = \"8.0.1\" enabled = false default_network_policy = true } karma = { chart_version = \"1.4.1\" version = \"v0.55\" enabled = false default_network_policy = true extra_values = <<EXTRA_VALUES ingress : enabled : false path : / annotations : kubernetes.io/ingress.class : nginx cert-manager.io/cluster-issuer : \"letsencrypt\" hosts : - karma.clusterfrak-dynamics.io env : - name : ALERTMANAGER_URI value : \"http://prometheus-operator-alertmanager.monitoring.svc.cluster.local:9093\" - name : ALERTMANAGER_PROXY value : \"true\" - name : FILTERS_DEFAULT value : \"@state=active severity!=info severity!=none\" EXTRA_VALUES } } Default charts values \u00b6 Some values are defined by default directly into the module. These can off course be overridden and or merged/replaced. You can find the defaults values in the upstream module . Eg. default values for cluster-autoscaler are in cluster-autoscaler.tf Overriding Helm provider values \u00b6 Helm provider have defaults values defined here : helm_defaults_defaults = { atomic = false cleanup_on_fail = false dependency_update = false disable_crd_hooks = false disable_webhooks = false force_update = false recreate_pods = false render_subchart_notes = true replace = false reset_values = false reuse_values = false skip_crds = false timeout = 3600 verify = false wait = true extra_values = \"\" } These can be overridden globally with the helm_defaults input variable or can be overridden per chart in terragrunt.hcl : helm_defaults = { replace = true verify = true timeout = 300 } cluster_autoscaler = { create_iam_resources_kiam = false create_iam_resources_irsa = true iam_policy_override = \"\" version = \"v1.14.7\" chart_version = \"6.4.0\" enabled = true default_network_policy = true cluster_name = dependency.eks.outputs.cluster_id timeout = 3600 <= here you can add any helm provider override } Overriding charts values.yaml \u00b6 It is possible to add or override values per charts. Helm provider use the same merge logic as Helm so you can basically rewrite the whole values.yaml if needed. Each chart has a extra_values variable where you can specify custom values. flux = { create_iam_resources_kiam = false create_iam_resources_irsa = true version = \"1.18.0\" chart_version = \"1.2.0\" enabled = false default_network_policy = true extra_values = <<EXTRA_VALUES git : url : \"ssh://git@gitlab.com/myrepo/gitops-${local.env}.git\" pollInterval : \"2m\" rbac : create : false registry : automationInterval : \"2m\" EXTRA_VALUES There are some examples in the terragrunt.hcl file. Not all the variables available are present. If you want a full list of variable, you can find them in the upstream module . For example for cluster-autoscaler you can see the default here .","title":"EKS Addons"},{"location":"user-guides/eks-addons/#eks-addons-module","text":"eks-addons is a custom module maintained here and provides either: helm v3 charts manifests operators For commonly used addons one Kubernetes and most specifically with EKS. The deployments are curated to be tightly integrated with AWS and EKS. The following addons are available and work out of the box.","title":"EKS addons module"},{"location":"user-guides/eks-addons/#helm-charts","text":"All charts have been tested with Helm v3 and the terraform-provider-helm v1.0 which supports Helm v3. They can be easily customize with custom values. cluster-autoscaler : scale worker nodes based on workload. external-dns : sync ingress and service records in route53. cert-manager : automatically generate TLS certificates, supports ACME v2. kiam : prevents pods to access EC2 metadata and enables pods to assume specific AWS IAM roles. nginx-ingress : processes Ingress object and acts as a HTTP/HTTPS proxy (compatible with cert-manager). metrics-server : enable metrics API and horizontal pod scaling (HPA). prometheus-operator : Monitoring / Alerting / Dashboards. fluentd-cloudwatch : forwards logs to AWS Cloudwatch. node-problem-detector : Forwards node problems to Kubernetes events flux : Continuous Delivery with Gitops workflow. sealed-secrets : Technology agnostic, store secrets on git. kong : API Gateway ingress controller. keycloak : Identity and access management","title":"Helm charts"},{"location":"user-guides/eks-addons/#kubernetes-manifests","text":"Kubernetes manifests are deployed with terraform-provider-kubectl cni-metrics-helper : Provides cloudwatch metrics for VPC CNI plugins.","title":"Kubernetes Manifests"},{"location":"user-guides/eks-addons/#operator","text":"Some project are transitioning to Operators . Istio is going to drop Helm support and is not compatible with Helm v3 so it has been removed and replaced with the Istio operator istio-operator : Service mesh for Kubernetes.","title":"Operator"},{"location":"user-guides/eks-addons/#iam-permissions","text":"Some addons require specific IAM permission. This can be done by either: IRSA: IAM role for service account which is the default and recommended way Kiam Addons that need IAM access have two variables: create_resources_irsa : default to true and uses IAM role for service account create_resources_kiam : default to false and uses KIAM to manage IAM permissions There is no specific config, everything is taken care of by the module.","title":"IAM permissions"},{"location":"user-guides/eks-addons/#customization","text":"All the configuration is done in eks-addons/terragrunt.hcl . include { path = \"${find_in_parent_folders()}\" } terraform { source = \"github.com/clusterfrak-dynamics/terraform-kubernetes-addons.git?ref=v5.0.0\" before_hook \"init\" { commands = [ \"init\" ] execute = [ \"bash\" , \"-c\" , \"wget -O terraform-provider-kubectl https://github.com/gavinbunney/terraform-provider-kubectl/releases/download/v1.2.1/terraform-provider-kubectl-linux-amd64 && chmod +x terraform-provider-kubectl\" ] } } locals { env = yamldecode(file( \"${get_terragrunt_dir()}/${find_in_parent_folders(\" common_tags.yaml \")}\" )) [ \"Env\" ] aws_region = yamldecode(file( \"${get_terragrunt_dir()}/${find_in_parent_folders(\" common_values.yaml \")}\" )) [ \"aws_region\" ] } dependency \"eks\" { config_path = \"../eks\" mock_outputs = { cluster_id = \"cluster-name\" cluster_oidc_issuer_url = \"https://oidc.eks.eu-west-3.amazonaws.com/id/0000000000000000\" } } dependency \"vpc\" { config_path = \"../vpc\" mock_outputs = { private_subnets_cidr_blocks = [ \"10.0.0.0/16\" , \"192.168.0.0/24\" ] } } inputs = { cluster-name = dependency.eks.outputs.cluster_id aws = { \"region\" = local.aws_region } eks = { \"cluster_oidc_issuer_url\" = dependency.eks.outputs.cluster_oidc_issuer_url } nginx_ingress = { version = \"0.29.0\" chart_version = \"1.31.0\" enabled = true default_network_policy = true ingress_cidr = \"0.0.0.0/0\" use_nlb = false use_l 7 = false } istio_operator = { enabled = false } cluster_autoscaler = { create_iam_resources_kiam = false create_iam_resources_irsa = true iam_policy_override = \"\" version = \"v1.14.7\" chart_version = \"6.4.0\" enabled = true default_network_policy = true cluster_name = dependency.eks.outputs.cluster_id } external_dns = { create_iam_resources_kiam = false create_iam_resources_irsa = true iam_policy_override = \"\" version = \"0.6.0-debian-10-r0\" chart_version = \"2.18.0\" enabled = true default_network_policy = true } cert_manager = { create_iam_resources_kiam = false create_iam_resources_irsa = true iam_policy_override = \"\" version = \"v0.13.1\" chart_version = \"v0.13.1\" enabled = true default_network_policy = true acme_email = \"kevin@particule.io\" enable_default_cluster_issuers = true allowed_cidrs = dependency.vpc.outputs.private_subnets_cidr_blocks } kiam = { create_iam_user = true create_iam_resources = true assume_role_policy_override = \"\" version = \"v3.5\" chart_version = \"5.7.0\" enabled = false default_network_policy = false iam_user = \"\" } metrics_server = { version = \"v0.3.6\" chart_version = \"2.9.0\" enabled = true default_network_policy = true allowed_cidrs = dependency.vpc.outputs.private_subnets_cidr_blocks } flux = { create_iam_resources_kiam = false create_iam_resources_irsa = true version = \"1.18.0\" chart_version = \"1.2.0\" enabled = false default_network_policy = true extra_values = <<EXTRA_VALUES git : url : \"ssh://git@gitlab.com/myrepo/gitops-${local.env}.git\" pollInterval : \"2m\" rbac : create : false registry : automationInterval : \"2m\" EXTRA_VALUES } prometheus_operator = { chart_version = \"8.7.0\" enabled = true default_network_policy = true allowed_cidrs = dependency.vpc.outputs.private_subnets_cidr_blocks extra_values = <<EXTRA_VALUES grafana : deploymentStrategy : type : Recreate ingress : enabled : false annotations : kubernetes.io/ingress.class : nginx cert-manager.io/cluster-issuer : \"letsencrypt\" hosts : - grafana.clusterfrak-dynamics.io tls : - secretName : grafana-clusterfrak-dynamics-io hosts : - grafana.clusterfrak-dynamics.io persistence : enabled : true storageClassName : gp 2 accessModes : - ReadWriteOnce size : 10 Gi prometheus : prometheusSpec : replicas : 1 retention : 180 d ruleSelectorNilUsesHelmValues : false ruleNamespaceSelector : any : true serviceMonitorSelectorNilUsesHelmValues : false serviceMonitorNamespaceSelector : any : true storageSpec : volumeClaimTemplate : spec : storageClassName : gp 2 accessModes : [ \"ReadWriteOnce\" ] resources : requests : storage : 50 Gi EXTRA_VALUES } fluentd_cloudwatch = { create_iam_resources_kiam = false create_iam_resources_irsa = true default_network_policy = true iam_policy_override = \"\" chart_version = \"0.12.1\" version = \"v1.7.4-debian-cloudwatch-1.0\" enabled = true containers_log_retention_in_days = 180 } npd = { chart_version = \"1.6.3\" version = \"v0.8.0\" enabled = true default_network_policy = true } sealed_secrets = { chart_version = \"1.7.6\" version = \"v0.9.7\" enabled = true default_network_policy = true } cni_metrics_helper = { create_iam_resources_kiam = false create_iam_resources_irsa = true enabled = true version = \"v1.5.5\" iam_policy_override = \"\" } kong = { version = \"1.4\" chart_version = \"1.2.0\" enabled = false default_network_policy = true ingress_cidr = \"0.0.0.0/0\" } keycloak = { chart_version = \"7.0.0\" version = \"8.0.1\" enabled = false default_network_policy = true } karma = { chart_version = \"1.4.1\" version = \"v0.55\" enabled = false default_network_policy = true extra_values = <<EXTRA_VALUES ingress : enabled : false path : / annotations : kubernetes.io/ingress.class : nginx cert-manager.io/cluster-issuer : \"letsencrypt\" hosts : - karma.clusterfrak-dynamics.io env : - name : ALERTMANAGER_URI value : \"http://prometheus-operator-alertmanager.monitoring.svc.cluster.local:9093\" - name : ALERTMANAGER_PROXY value : \"true\" - name : FILTERS_DEFAULT value : \"@state=active severity!=info severity!=none\" EXTRA_VALUES } }","title":"Customization"},{"location":"user-guides/eks-addons/#default-charts-values","text":"Some values are defined by default directly into the module. These can off course be overridden and or merged/replaced. You can find the defaults values in the upstream module . Eg. default values for cluster-autoscaler are in cluster-autoscaler.tf","title":"Default charts values"},{"location":"user-guides/eks-addons/#overriding-helm-provider-values","text":"Helm provider have defaults values defined here : helm_defaults_defaults = { atomic = false cleanup_on_fail = false dependency_update = false disable_crd_hooks = false disable_webhooks = false force_update = false recreate_pods = false render_subchart_notes = true replace = false reset_values = false reuse_values = false skip_crds = false timeout = 3600 verify = false wait = true extra_values = \"\" } These can be overridden globally with the helm_defaults input variable or can be overridden per chart in terragrunt.hcl : helm_defaults = { replace = true verify = true timeout = 300 } cluster_autoscaler = { create_iam_resources_kiam = false create_iam_resources_irsa = true iam_policy_override = \"\" version = \"v1.14.7\" chart_version = \"6.4.0\" enabled = true default_network_policy = true cluster_name = dependency.eks.outputs.cluster_id timeout = 3600 <= here you can add any helm provider override }","title":"Overriding Helm provider values"},{"location":"user-guides/eks-addons/#overriding-charts-valuesyaml","text":"It is possible to add or override values per charts. Helm provider use the same merge logic as Helm so you can basically rewrite the whole values.yaml if needed. Each chart has a extra_values variable where you can specify custom values. flux = { create_iam_resources_kiam = false create_iam_resources_irsa = true version = \"1.18.0\" chart_version = \"1.2.0\" enabled = false default_network_policy = true extra_values = <<EXTRA_VALUES git : url : \"ssh://git@gitlab.com/myrepo/gitops-${local.env}.git\" pollInterval : \"2m\" rbac : create : false registry : automationInterval : \"2m\" EXTRA_VALUES There are some examples in the terragrunt.hcl file. Not all the variables available are present. If you want a full list of variable, you can find them in the upstream module . For example for cluster-autoscaler you can see the default here .","title":"Overriding charts values.yaml"},{"location":"user-guides/eks-namespaces/","text":"EKS namespaces module \u00b6 eks-namespace is a custom module maintained here and allow to create and manage Kubernetes namespaces with Terragrunt/Terraform. It provides: Namespaces quotas Namesapces limitranges Customization \u00b6 Just like the other modules, custom configuration is done in terragrunt.hcl . It takes a list of namespaces with their configuration as an input variable. include { path = \"${find_in_parent_folders()}\" } dependencies { paths = [ \"../eks\" ] } terraform { source = \"github.com/clusterfrak-dynamics/terraform-kubernetes-namespaces.git?ref=v4.0.1\" } locals { aws_region = yamldecode(file( \"${get_terragrunt_dir()}/${find_in_parent_folders(\" common_values.yaml \")}\" )) [ \"aws_region\" ] env = yamldecode(file( \"${get_terragrunt_dir()}/${find_in_parent_folders(\" common_tags.yaml \")}\" )) [ \"Env\" ] } dependency \"eks\" { config_path = \"../eks\" mock_outputs = { cluster_id = \"cluster-name\" } } inputs = { aws = { \"region\" = local.aws_region } eks = { \"cluster_name\" = dependency.eks.outputs.cluster_id } // // [ env ] // env = local.env // // [ namespaces ] // namespaces = [ { \"name\" = \"myapp-${local.env}\" \"kiam_allowed_regexp\" = \"^$\" \"requests.cpu\" = \"50\" \"requests.memory\" = \"10Gi\" \"pods\" = \"100\" \"count/cronjobs.batch\" = \"100\" \"count/ingresses.extensions\" = \"5\" \"requests.nvidia.com/gpu\" = \"0\" \"services.loadbalancers\" = \"0\" \"services.nodeports\" = \"0\" \"services\" = \"10\" }, ] }","title":"EKS Namespaces"},{"location":"user-guides/eks-namespaces/#eks-namespaces-module","text":"eks-namespace is a custom module maintained here and allow to create and manage Kubernetes namespaces with Terragrunt/Terraform. It provides: Namespaces quotas Namesapces limitranges","title":"EKS namespaces module"},{"location":"user-guides/eks-namespaces/#customization","text":"Just like the other modules, custom configuration is done in terragrunt.hcl . It takes a list of namespaces with their configuration as an input variable. include { path = \"${find_in_parent_folders()}\" } dependencies { paths = [ \"../eks\" ] } terraform { source = \"github.com/clusterfrak-dynamics/terraform-kubernetes-namespaces.git?ref=v4.0.1\" } locals { aws_region = yamldecode(file( \"${get_terragrunt_dir()}/${find_in_parent_folders(\" common_values.yaml \")}\" )) [ \"aws_region\" ] env = yamldecode(file( \"${get_terragrunt_dir()}/${find_in_parent_folders(\" common_tags.yaml \")}\" )) [ \"Env\" ] } dependency \"eks\" { config_path = \"../eks\" mock_outputs = { cluster_id = \"cluster-name\" } } inputs = { aws = { \"region\" = local.aws_region } eks = { \"cluster_name\" = dependency.eks.outputs.cluster_id } // // [ env ] // env = local.env // // [ namespaces ] // namespaces = [ { \"name\" = \"myapp-${local.env}\" \"kiam_allowed_regexp\" = \"^$\" \"requests.cpu\" = \"50\" \"requests.memory\" = \"10Gi\" \"pods\" = \"100\" \"count/cronjobs.batch\" = \"100\" \"count/ingresses.extensions\" = \"5\" \"requests.nvidia.com/gpu\" = \"0\" \"services.loadbalancers\" = \"0\" \"services.nodeports\" = \"0\" \"services\" = \"10\" }, ] }","title":"Customization"},{"location":"user-guides/eks/","text":"EKS module \u00b6 Upstream configuration \u00b6 EKS module is also upstream and allow to deploy an EKS cluster which supports: managed node pools launch configuration node pools launch template node pools tEKS uses launch template by default and use one node pool per availability zone. You can use any inputs from the upstream module to configure the cluster in eks/terragrunt.hcl . include { path = \"${find_in_parent_folders()}\" } terraform { source = \"github.com/terraform-aws-modules/terraform-aws-eks?ref=v8.2.0\" before_hook \"init\" { commands = [ \"init\" ] execute = [ \"bash\" , \"-c\" , \"wget -O terraform-provider-kubectl https://github.com/gavinbunney/terraform-provider-kubectl/releases/download/v1.2.1/terraform-provider-kubectl-linux-amd64 && chmod +x terraform-provider-kubectl\" ] } after_hook \"kubeconfig\" { commands = [ \"apply\" ] execute = [ \"bash\" , \"-c\" , \"terraform output kubeconfig 2>/dev/null > ${get_terragrunt_dir()}/kubeconfig\" ] } after_hook \"kube-system-label\" { commands = [ \"apply\" ] execute = [ \"bash\" , \"-c\" , \"kubectl --kubeconfig kubeconfig label ns kube-system name=kube-system --overwrite\" ] } after_hook \"remove-default-psp\" { commands = [ \"apply\" ] execute = [ \"bash\" , \"-c\" , \"kubectl --kubeconfig kubeconfig delete psp eks.privileged || true\" ] } after_hook \"remove-default-psp-clusterrolebindind\" { commands = [ \"apply\" ] execute = [ \"bash\" , \"-c\" , \"kubectl --kubeconfig kubeconfig delete clusterrolebinding eks:podsecuritypolicy:authenticated || true\" ] } after_hook \"remove-default-psp-clusterrole\" { commands = [ \"apply\" ] execute = [ \"bash\" , \"-c\" , \"kubectl --kubeconfig kubeconfig delete clusterrole eks:podsecuritypolicy:privileged || true\" ] } } locals { aws_region = yamldecode(file( \"${get_terragrunt_dir()}/${find_in_parent_folders(\" common_values.yaml \")}\" )) [ \"aws_region\" ] env = yamldecode(file( \"${get_terragrunt_dir()}/${find_in_parent_folders(\" common_tags.yaml \")}\" )) [ \"Env\" ] aws_account_id = yamldecode(file( \"${get_terragrunt_dir()}/${find_in_parent_folders(\" common_values.yaml \")}\" )) [ \"aws_account_id\" ] custom_tags = yamldecode(file( \"${get_terragrunt_dir()}/${find_in_parent_folders(\" common_tags.yaml \")}\" )) prefix = yamldecode(file( \"${get_terragrunt_dir()}/${find_in_parent_folders(\" common_values.yaml \")}\" )) [ \"prefix\" ] cluster_name = \"eks-${local.prefix}-${local.env}\" } dependency \"vpc\" { config_path = \"../vpc\" mock_outputs = { vpc_id = \"vpc-00000000\" private_subnets = [ \"subnet-00000000\" , \"subnet-00000001\" , \"subnet-00000002\" , ] } } inputs = { aws = { \"region\" = local.aws_region } psp_privileged_ns = [ \"cluster-autoscaler\" , #waiting for https://github.com/helm/charts/pull/ 20891 \"istio-system\" #istio does not support psp by default ] tags = merge( local.custom_tags ) cluster_name = local.cluster_name subnets = dependency.vpc.outputs.private_subnets vpc_id = dependency.vpc.outputs.vpc_id write_kubeconfig = false enable_irsa = true kubeconfig_aws_authenticator_additional_args = [] cluster_version = \"1.14\" cluster_enabled_log_types = [ \"api\" , \"audit\" , \"authenticator\" , \"controllerManager\" , \"scheduler\" ] manage_worker_autoscaling_policy = false worker_groups_launch_template = [ { name = \"default-${local.aws_region}a\" instance_type = \"t3.medium\" asg_min_size = 1 asg_max_size = 3 asg_desired_capacity = 1 subnets = [ dependency.vpc.outputs.private_subnets [ 0 ]] autoscaling_enabled = true root_volume_size = 50 tags = [ { key = \"CLUSTER_ID\" value = local.cluster_name propagate_at_launch = true }, ] }, { name = \"default-${local.aws_region}b\" instance_type = \"t3.medium\" asg_min_size = 0 asg_max_size = 3 asg_desired_capacity = 0 subnets = [ dependency.vpc.outputs.private_subnets [ 1 ]] autoscaling_enabled = true root_volume_size = 50 tags = [ { key = \"CLUSTER_ID\" value = local.cluster_name propagate_at_launch = true }, ] }, { name = \"default-${local.aws_region}c\" instance_type = \"t3.medium\" asg_min_size = 0 asg_max_size = 3 asg_desired_capacity = 0 subnets = [ dependency.vpc.outputs.private_subnets [ 2 ]] autoscaling_enabled = true root_volume_size = 50 tags = [ { key = \"CLUSTER_ID\" value = local.cluster_name propagate_at_launch = true }, ] }, ] } Customizations \u00b6 Besides the upstream module, there are some customizations. If you look at the directory structure: . \u251c\u2500\u2500 manifests \u2502 \u251c\u2500\u2500 calico.yaml \u2502 \u251c\u2500\u2500 psp-default-clusterrole.yaml \u2502 \u251c\u2500\u2500 psp-default-clusterrolebinding.yaml \u2502 \u251c\u2500\u2500 psp-default.yaml \u2502 \u251c\u2500\u2500 psp-privileged-clusterrole.yaml \u2502 \u251c\u2500\u2500 psp-privileged-clusterrolebinding.yaml \u2502 \u251c\u2500\u2500 psp-privileged-node-rolebinding.yaml \u2502 \u2514\u2500\u2500 psp-privileged.yaml \u251c\u2500\u2500 manifests.tf \u251c\u2500\u2500 providers.tf \u2514\u2500\u2500 terragrunt.hcl Terragrunt hooks \u00b6 In addition to the upstream module there is some hooks included, these hooks can be remove if necessary. In order: Download the terraform-provider-kubectl to manage manifests (necessary if using `manifests.tf) Copy the kubeconfig locally Label the kube-system namespace with its name Remove the defaults EKS podSecurityPolicies (see #401 ) Custom manifests \u00b6 terraform-provider-kubectl allows to deploy Kubernetes manifests in a cleaner way than using a local-exec . manifests.tf is parsing the manifest folder and applying them to the cluster. Calico \u00b6 Calico is used to enable network policies enforcement on the cluster. To disable calico you can remove the calico.yaml file. Pod Security Policies \u00b6 The default EKS Pod Security Policies is privileged. By default it is removed (by the previous hooks) and more sensible non privileged default pod security policies are deployed: all the service account in kube-system can use privileged --- apiVersion : policy/v1beta1 kind : PodSecurityPolicy metadata : name : privileged annotations : seccomp.security.alpha.kubernetes.io/allowedProfileNames : '*' spec : privileged : true allowPrivilegeEscalation : true allowedCapabilities : [ '*' ] volumes : [ '*' ] hostNetwork : true hostPorts : - min : 0 max : 65535 hostIPC : true hostPID : true runAsUser : rule : 'RunAsAny' seLinux : rule : 'RunAsAny' supplementalGroups : rule : 'RunAsAny' fsGroup : rule : 'RunAsAny' any other authenticated user can use default --- apiVersion : policy/v1beta1 kind : PodSecurityPolicy metadata : name : default annotations : seccomp.security.alpha.kubernetes.io/allowedProfileNames : 'docker/default' seccomp.security.alpha.kubernetes.io/defaultProfileName : 'docker/default' spec : privileged : false allowPrivilegeEscalation : false allowedCapabilities : [] # default set of capabilities are implicitly allowed volumes : - 'configMap' - 'emptyDir' - 'projected' - 'secret' - 'downwardAPI' - 'persistentVolumeClaim' hostNetwork : false hostIPC : false hostPID : false runAsUser : rule : 'MustRunAsNonRoot' runAsGroup : rule : 'MustRunAs' ranges : - min : 1 max : 65535 seLinux : rule : 'RunAsAny' supplementalGroups : rule : 'MustRunAs' ranges : - min : 1 max : 65535 fsGroup : rule : 'MustRunAs' ranges : - min : 1 max : 65535 The input variable psp_privileged_ns allow to give privileged to services account inside a namespace. Eg. in terragrunt.hcl : psp_privileged_ns = [ \"cluster-autoscaler\" , #waiting for https://github.com/helm/charts/pull/ 20891 \"istio-system\" #istio does not support psp by default ] This gives to all the service accounts inside cluster-autoscaler and istio-system access to the privileged pod security policy. \u00b6","title":"EKS"},{"location":"user-guides/eks/#eks-module","text":"","title":"EKS module"},{"location":"user-guides/eks/#upstream-configuration","text":"EKS module is also upstream and allow to deploy an EKS cluster which supports: managed node pools launch configuration node pools launch template node pools tEKS uses launch template by default and use one node pool per availability zone. You can use any inputs from the upstream module to configure the cluster in eks/terragrunt.hcl . include { path = \"${find_in_parent_folders()}\" } terraform { source = \"github.com/terraform-aws-modules/terraform-aws-eks?ref=v8.2.0\" before_hook \"init\" { commands = [ \"init\" ] execute = [ \"bash\" , \"-c\" , \"wget -O terraform-provider-kubectl https://github.com/gavinbunney/terraform-provider-kubectl/releases/download/v1.2.1/terraform-provider-kubectl-linux-amd64 && chmod +x terraform-provider-kubectl\" ] } after_hook \"kubeconfig\" { commands = [ \"apply\" ] execute = [ \"bash\" , \"-c\" , \"terraform output kubeconfig 2>/dev/null > ${get_terragrunt_dir()}/kubeconfig\" ] } after_hook \"kube-system-label\" { commands = [ \"apply\" ] execute = [ \"bash\" , \"-c\" , \"kubectl --kubeconfig kubeconfig label ns kube-system name=kube-system --overwrite\" ] } after_hook \"remove-default-psp\" { commands = [ \"apply\" ] execute = [ \"bash\" , \"-c\" , \"kubectl --kubeconfig kubeconfig delete psp eks.privileged || true\" ] } after_hook \"remove-default-psp-clusterrolebindind\" { commands = [ \"apply\" ] execute = [ \"bash\" , \"-c\" , \"kubectl --kubeconfig kubeconfig delete clusterrolebinding eks:podsecuritypolicy:authenticated || true\" ] } after_hook \"remove-default-psp-clusterrole\" { commands = [ \"apply\" ] execute = [ \"bash\" , \"-c\" , \"kubectl --kubeconfig kubeconfig delete clusterrole eks:podsecuritypolicy:privileged || true\" ] } } locals { aws_region = yamldecode(file( \"${get_terragrunt_dir()}/${find_in_parent_folders(\" common_values.yaml \")}\" )) [ \"aws_region\" ] env = yamldecode(file( \"${get_terragrunt_dir()}/${find_in_parent_folders(\" common_tags.yaml \")}\" )) [ \"Env\" ] aws_account_id = yamldecode(file( \"${get_terragrunt_dir()}/${find_in_parent_folders(\" common_values.yaml \")}\" )) [ \"aws_account_id\" ] custom_tags = yamldecode(file( \"${get_terragrunt_dir()}/${find_in_parent_folders(\" common_tags.yaml \")}\" )) prefix = yamldecode(file( \"${get_terragrunt_dir()}/${find_in_parent_folders(\" common_values.yaml \")}\" )) [ \"prefix\" ] cluster_name = \"eks-${local.prefix}-${local.env}\" } dependency \"vpc\" { config_path = \"../vpc\" mock_outputs = { vpc_id = \"vpc-00000000\" private_subnets = [ \"subnet-00000000\" , \"subnet-00000001\" , \"subnet-00000002\" , ] } } inputs = { aws = { \"region\" = local.aws_region } psp_privileged_ns = [ \"cluster-autoscaler\" , #waiting for https://github.com/helm/charts/pull/ 20891 \"istio-system\" #istio does not support psp by default ] tags = merge( local.custom_tags ) cluster_name = local.cluster_name subnets = dependency.vpc.outputs.private_subnets vpc_id = dependency.vpc.outputs.vpc_id write_kubeconfig = false enable_irsa = true kubeconfig_aws_authenticator_additional_args = [] cluster_version = \"1.14\" cluster_enabled_log_types = [ \"api\" , \"audit\" , \"authenticator\" , \"controllerManager\" , \"scheduler\" ] manage_worker_autoscaling_policy = false worker_groups_launch_template = [ { name = \"default-${local.aws_region}a\" instance_type = \"t3.medium\" asg_min_size = 1 asg_max_size = 3 asg_desired_capacity = 1 subnets = [ dependency.vpc.outputs.private_subnets [ 0 ]] autoscaling_enabled = true root_volume_size = 50 tags = [ { key = \"CLUSTER_ID\" value = local.cluster_name propagate_at_launch = true }, ] }, { name = \"default-${local.aws_region}b\" instance_type = \"t3.medium\" asg_min_size = 0 asg_max_size = 3 asg_desired_capacity = 0 subnets = [ dependency.vpc.outputs.private_subnets [ 1 ]] autoscaling_enabled = true root_volume_size = 50 tags = [ { key = \"CLUSTER_ID\" value = local.cluster_name propagate_at_launch = true }, ] }, { name = \"default-${local.aws_region}c\" instance_type = \"t3.medium\" asg_min_size = 0 asg_max_size = 3 asg_desired_capacity = 0 subnets = [ dependency.vpc.outputs.private_subnets [ 2 ]] autoscaling_enabled = true root_volume_size = 50 tags = [ { key = \"CLUSTER_ID\" value = local.cluster_name propagate_at_launch = true }, ] }, ] }","title":"Upstream configuration"},{"location":"user-guides/eks/#customizations","text":"Besides the upstream module, there are some customizations. If you look at the directory structure: . \u251c\u2500\u2500 manifests \u2502 \u251c\u2500\u2500 calico.yaml \u2502 \u251c\u2500\u2500 psp-default-clusterrole.yaml \u2502 \u251c\u2500\u2500 psp-default-clusterrolebinding.yaml \u2502 \u251c\u2500\u2500 psp-default.yaml \u2502 \u251c\u2500\u2500 psp-privileged-clusterrole.yaml \u2502 \u251c\u2500\u2500 psp-privileged-clusterrolebinding.yaml \u2502 \u251c\u2500\u2500 psp-privileged-node-rolebinding.yaml \u2502 \u2514\u2500\u2500 psp-privileged.yaml \u251c\u2500\u2500 manifests.tf \u251c\u2500\u2500 providers.tf \u2514\u2500\u2500 terragrunt.hcl","title":"Customizations"},{"location":"user-guides/eks/#terragrunt-hooks","text":"In addition to the upstream module there is some hooks included, these hooks can be remove if necessary. In order: Download the terraform-provider-kubectl to manage manifests (necessary if using `manifests.tf) Copy the kubeconfig locally Label the kube-system namespace with its name Remove the defaults EKS podSecurityPolicies (see #401 )","title":"Terragrunt hooks"},{"location":"user-guides/eks/#custom-manifests","text":"terraform-provider-kubectl allows to deploy Kubernetes manifests in a cleaner way than using a local-exec . manifests.tf is parsing the manifest folder and applying them to the cluster.","title":"Custom manifests"},{"location":"user-guides/eks/#calico","text":"Calico is used to enable network policies enforcement on the cluster. To disable calico you can remove the calico.yaml file.","title":"Calico"},{"location":"user-guides/eks/#pod-security-policies","text":"The default EKS Pod Security Policies is privileged. By default it is removed (by the previous hooks) and more sensible non privileged default pod security policies are deployed: all the service account in kube-system can use privileged --- apiVersion : policy/v1beta1 kind : PodSecurityPolicy metadata : name : privileged annotations : seccomp.security.alpha.kubernetes.io/allowedProfileNames : '*' spec : privileged : true allowPrivilegeEscalation : true allowedCapabilities : [ '*' ] volumes : [ '*' ] hostNetwork : true hostPorts : - min : 0 max : 65535 hostIPC : true hostPID : true runAsUser : rule : 'RunAsAny' seLinux : rule : 'RunAsAny' supplementalGroups : rule : 'RunAsAny' fsGroup : rule : 'RunAsAny' any other authenticated user can use default --- apiVersion : policy/v1beta1 kind : PodSecurityPolicy metadata : name : default annotations : seccomp.security.alpha.kubernetes.io/allowedProfileNames : 'docker/default' seccomp.security.alpha.kubernetes.io/defaultProfileName : 'docker/default' spec : privileged : false allowPrivilegeEscalation : false allowedCapabilities : [] # default set of capabilities are implicitly allowed volumes : - 'configMap' - 'emptyDir' - 'projected' - 'secret' - 'downwardAPI' - 'persistentVolumeClaim' hostNetwork : false hostIPC : false hostPID : false runAsUser : rule : 'MustRunAsNonRoot' runAsGroup : rule : 'MustRunAs' ranges : - min : 1 max : 65535 seLinux : rule : 'RunAsAny' supplementalGroups : rule : 'MustRunAs' ranges : - min : 1 max : 65535 fsGroup : rule : 'MustRunAs' ranges : - min : 1 max : 65535 The input variable psp_privileged_ns allow to give privileged to services account inside a namespace. Eg. in terragrunt.hcl : psp_privileged_ns = [ \"cluster-autoscaler\" , #waiting for https://github.com/helm/charts/pull/ 20891 \"istio-system\" #istio does not support psp by default ] This gives to all the service accounts inside cluster-autoscaler and istio-system access to the privileged pod security policy.","title":"Pod Security Policies"},{"location":"user-guides/getting-started/","text":"Getting started \u00b6 Tooling requirements \u00b6 The necessary tools are in requirements.yaml you can install them any way you want, make sure they are available in your $PATH. The following dependencies are required on the deployer host: Terraform Terragrunt kubectl helm aws-iam-authenticator AWS requirements \u00b6 At least one AWS account awscli configured ( see installation instructions ) to access your AWS account. A route53 hosted zone if you plan to use external-dns or cert-manager but it is not a hard requirement. Getting the template repository \u00b6 You can either clone the repo locally or generate/fork a template from github. git clone https://github.com/clusterfrak-dynamics/teks.git --branch v6.0.0 The terraform directory structure is the following: . \u2514\u2500\u2500 live \u2514\u2500\u2500 demo \u251c\u2500\u2500 common_tags.yaml \u251c\u2500\u2500 common_values.yaml \u251c\u2500\u2500 eu-west-3 \u2502 \u251c\u2500\u2500 ecr \u2502 \u2502 \u251c\u2500\u2500 provider.tf \u2502 \u2502 \u2514\u2500\u2500 terragrunt.hcl \u2502 \u251c\u2500\u2500 eks \u2502 \u2502 \u251c\u2500\u2500 kubeconfig \u2502 \u2502 \u251c\u2500\u2500 manifests \u2502 \u2502 \u2502 \u251c\u2500\u2500 calico.yaml \u2502 \u2502 \u2502 \u251c\u2500\u2500 psp-default-clusterrole.yaml \u2502 \u2502 \u2502 \u251c\u2500\u2500 psp-default-clusterrolebinding.yaml \u2502 \u2502 \u2502 \u251c\u2500\u2500 psp-default.yaml \u2502 \u2502 \u2502 \u251c\u2500\u2500 psp-privileged-clusterrole.yaml \u2502 \u2502 \u2502 \u251c\u2500\u2500 psp-privileged-clusterrolebinding.yaml \u2502 \u2502 \u2502 \u251c\u2500\u2500 psp-privileged-node-rolebinding.yaml \u2502 \u2502 \u2502 \u2514\u2500\u2500 psp-privileged.yaml \u2502 \u2502 \u251c\u2500\u2500 manifests.tf \u2502 \u2502 \u251c\u2500\u2500 providers.tf \u2502 \u2502 \u2514\u2500\u2500 terragrunt.hcl \u2502 \u251c\u2500\u2500 eks-addons \u2502 \u2502 \u251c\u2500\u2500 examples \u2502 \u2502 \u2502 \u251c\u2500\u2500 keycloak-values.yaml \u2502 \u2502 \u2502 \u2514\u2500\u2500 kong-values.yaml \u2502 \u2502 \u251c\u2500\u2500 providers.tf \u2502 \u2502 \u2514\u2500\u2500 terragrunt.hcl \u2502 \u251c\u2500\u2500 eks-namespaces \u2502 \u2502 \u251c\u2500\u2500 providers.tf \u2502 \u2502 \u2514\u2500\u2500 terragrunt.hcl \u2502 \u2514\u2500\u2500 vpc \u2502 \u251c\u2500\u2500 provider.tf \u2502 \u2514\u2500\u2500 terragrunt.hcl \u2514\u2500\u2500 terragrunt.hcl Each cluster in inside the terraform/live folder and then modules are group by AWS region. Start a new cluster \u00b6 Create a new cluster beside demo : cp -ar demo mycluster Configuring Terragrunt remote state \u00b6 live/demo/terragrunt.hcl is the parent terragrunt file use to configure remote state. The configuration is done automatically from the common_values.yaml file. --- aws_account_id : 161285725140 aws_region : eu-west-3 prefix : cfd-particule The values here will generate automatically the parent terragrunt file. remote_state { backend = \"s3\" config = { bucket = \"${yamldecode(file(\" common_values.yaml \"))[\" prefix \"]}-tf-state-store-${yamldecode(file(\" common_tags.yaml \"))[\" Env \"]}-${yamldecode(file(\" common_values.yaml \"))[\" aws_region \"]}-particule\" key = \"${path_relative_to_include()}\" region = \"${yamldecode(file(\" common_values.yaml \"))[\" aws_region \"]}\" encrypt = true dynamodb_table = \"${yamldecode(file(\" common_values.yaml \"))[\" prefix \"]}-tf-state-store-lock-${yamldecode(file(\" common_tags.yaml \"))[\" Env \"]}-${yamldecode(file(\" common_values.yaml \"))[\" aws_region \"]}-particule\" } } #iam_role = \"arn:aws:iam::${yamldecode(file(\" common_values.yaml \"))[\" aws_account_id \"]}:role/administrator\" You can either customize the values or edit directly the terragrunt.hcl file. Running Terragrunt command \u00b6 Terragrunt command are run inside their respective folder, for example, to run the vpc module: cd vpc terragrunt apply","title":"Getting started"},{"location":"user-guides/getting-started/#getting-started","text":"","title":"Getting started"},{"location":"user-guides/getting-started/#tooling-requirements","text":"The necessary tools are in requirements.yaml you can install them any way you want, make sure they are available in your $PATH. The following dependencies are required on the deployer host: Terraform Terragrunt kubectl helm aws-iam-authenticator","title":"Tooling requirements"},{"location":"user-guides/getting-started/#aws-requirements","text":"At least one AWS account awscli configured ( see installation instructions ) to access your AWS account. A route53 hosted zone if you plan to use external-dns or cert-manager but it is not a hard requirement.","title":"AWS requirements"},{"location":"user-guides/getting-started/#getting-the-template-repository","text":"You can either clone the repo locally or generate/fork a template from github. git clone https://github.com/clusterfrak-dynamics/teks.git --branch v6.0.0 The terraform directory structure is the following: . \u2514\u2500\u2500 live \u2514\u2500\u2500 demo \u251c\u2500\u2500 common_tags.yaml \u251c\u2500\u2500 common_values.yaml \u251c\u2500\u2500 eu-west-3 \u2502 \u251c\u2500\u2500 ecr \u2502 \u2502 \u251c\u2500\u2500 provider.tf \u2502 \u2502 \u2514\u2500\u2500 terragrunt.hcl \u2502 \u251c\u2500\u2500 eks \u2502 \u2502 \u251c\u2500\u2500 kubeconfig \u2502 \u2502 \u251c\u2500\u2500 manifests \u2502 \u2502 \u2502 \u251c\u2500\u2500 calico.yaml \u2502 \u2502 \u2502 \u251c\u2500\u2500 psp-default-clusterrole.yaml \u2502 \u2502 \u2502 \u251c\u2500\u2500 psp-default-clusterrolebinding.yaml \u2502 \u2502 \u2502 \u251c\u2500\u2500 psp-default.yaml \u2502 \u2502 \u2502 \u251c\u2500\u2500 psp-privileged-clusterrole.yaml \u2502 \u2502 \u2502 \u251c\u2500\u2500 psp-privileged-clusterrolebinding.yaml \u2502 \u2502 \u2502 \u251c\u2500\u2500 psp-privileged-node-rolebinding.yaml \u2502 \u2502 \u2502 \u2514\u2500\u2500 psp-privileged.yaml \u2502 \u2502 \u251c\u2500\u2500 manifests.tf \u2502 \u2502 \u251c\u2500\u2500 providers.tf \u2502 \u2502 \u2514\u2500\u2500 terragrunt.hcl \u2502 \u251c\u2500\u2500 eks-addons \u2502 \u2502 \u251c\u2500\u2500 examples \u2502 \u2502 \u2502 \u251c\u2500\u2500 keycloak-values.yaml \u2502 \u2502 \u2502 \u2514\u2500\u2500 kong-values.yaml \u2502 \u2502 \u251c\u2500\u2500 providers.tf \u2502 \u2502 \u2514\u2500\u2500 terragrunt.hcl \u2502 \u251c\u2500\u2500 eks-namespaces \u2502 \u2502 \u251c\u2500\u2500 providers.tf \u2502 \u2502 \u2514\u2500\u2500 terragrunt.hcl \u2502 \u2514\u2500\u2500 vpc \u2502 \u251c\u2500\u2500 provider.tf \u2502 \u2514\u2500\u2500 terragrunt.hcl \u2514\u2500\u2500 terragrunt.hcl Each cluster in inside the terraform/live folder and then modules are group by AWS region.","title":"Getting the template repository"},{"location":"user-guides/getting-started/#start-a-new-cluster","text":"Create a new cluster beside demo : cp -ar demo mycluster","title":"Start a new cluster"},{"location":"user-guides/getting-started/#configuring-terragrunt-remote-state","text":"live/demo/terragrunt.hcl is the parent terragrunt file use to configure remote state. The configuration is done automatically from the common_values.yaml file. --- aws_account_id : 161285725140 aws_region : eu-west-3 prefix : cfd-particule The values here will generate automatically the parent terragrunt file. remote_state { backend = \"s3\" config = { bucket = \"${yamldecode(file(\" common_values.yaml \"))[\" prefix \"]}-tf-state-store-${yamldecode(file(\" common_tags.yaml \"))[\" Env \"]}-${yamldecode(file(\" common_values.yaml \"))[\" aws_region \"]}-particule\" key = \"${path_relative_to_include()}\" region = \"${yamldecode(file(\" common_values.yaml \"))[\" aws_region \"]}\" encrypt = true dynamodb_table = \"${yamldecode(file(\" common_values.yaml \"))[\" prefix \"]}-tf-state-store-lock-${yamldecode(file(\" common_tags.yaml \"))[\" Env \"]}-${yamldecode(file(\" common_values.yaml \"))[\" aws_region \"]}-particule\" } } #iam_role = \"arn:aws:iam::${yamldecode(file(\" common_values.yaml \"))[\" aws_account_id \"]}:role/administrator\" You can either customize the values or edit directly the terragrunt.hcl file.","title":"Configuring Terragrunt remote state"},{"location":"user-guides/getting-started/#running-terragrunt-command","text":"Terragrunt command are run inside their respective folder, for example, to run the vpc module: cd vpc terragrunt apply","title":"Running Terragrunt command"},{"location":"user-guides/vpc/","text":"VPC module \u00b6 The vpc module is the one from upstream . To customize it. Modify the vpc/terragrunt.hcl file. You can use any inputs available in the upstream module. include { path = \"${find_in_parent_folders()}\" } terraform { source = \"github.com/terraform-aws-modules/terraform-aws-vpc?ref=v2.24.0\" } locals { aws_region = yamldecode(file( \"${get_terragrunt_dir()}/${find_in_parent_folders(\" common_values.yaml \")}\" )) [ \"aws_region\" ] env = yamldecode(file( \"${get_terragrunt_dir()}/${find_in_parent_folders(\" common_tags.yaml \")}\" )) [ \"Env\" ] custom_tags = yamldecode(file( \"${get_terragrunt_dir()}/${find_in_parent_folders(\" common_tags.yaml \")}\" )) prefix = yamldecode(file( \"${get_terragrunt_dir()}/${find_in_parent_folders(\" common_values.yaml \")}\" )) [ \"prefix\" ] } inputs = { aws = { \"region\" = local.aws_region } tags = merge( { \"kubernetes.io/cluster/eks-${local.prefix}-${local.env}\" = \"shared\" } , local.custom_tags ) name = \"vpc-eks-${local.env}\" cidr = \"10.0.0.0/16\" azs = [ \"${local.aws_region}a\" , \"${local.aws_region}b\" , \"${local.aws_region}c\" ] private_subnets = [ \"10.0.1.0/24\" , \"10.0.2.0/24\" , \"10.0.3.0/24\" ] public_subnets = [ \"10.0.101.0/24\" , \"10.0.102.0/24\" , \"10.0.103.0/24\" ] assign_generated_ipv 6 _cidr_block = true enable_nat_gateway = true single_nat_gateway = true enable_dns_hostnames = true enable_dns_support = true public_subnet_tags = { \"kubernetes.io/cluster/eks-${local.prefix}-${local.env}\" = \"shared\" \"kubernetes.io/role/elb\" = \"1\" } private_subnet_tags = { \"kubernetes.io/cluster/eks-${local.prefix}-${local.env}\" = \"shared\" \"kubernetes.io/role/internal-elb\" = \"1\" } }","title":"VPC"},{"location":"user-guides/vpc/#vpc-module","text":"The vpc module is the one from upstream . To customize it. Modify the vpc/terragrunt.hcl file. You can use any inputs available in the upstream module. include { path = \"${find_in_parent_folders()}\" } terraform { source = \"github.com/terraform-aws-modules/terraform-aws-vpc?ref=v2.24.0\" } locals { aws_region = yamldecode(file( \"${get_terragrunt_dir()}/${find_in_parent_folders(\" common_values.yaml \")}\" )) [ \"aws_region\" ] env = yamldecode(file( \"${get_terragrunt_dir()}/${find_in_parent_folders(\" common_tags.yaml \")}\" )) [ \"Env\" ] custom_tags = yamldecode(file( \"${get_terragrunt_dir()}/${find_in_parent_folders(\" common_tags.yaml \")}\" )) prefix = yamldecode(file( \"${get_terragrunt_dir()}/${find_in_parent_folders(\" common_values.yaml \")}\" )) [ \"prefix\" ] } inputs = { aws = { \"region\" = local.aws_region } tags = merge( { \"kubernetes.io/cluster/eks-${local.prefix}-${local.env}\" = \"shared\" } , local.custom_tags ) name = \"vpc-eks-${local.env}\" cidr = \"10.0.0.0/16\" azs = [ \"${local.aws_region}a\" , \"${local.aws_region}b\" , \"${local.aws_region}c\" ] private_subnets = [ \"10.0.1.0/24\" , \"10.0.2.0/24\" , \"10.0.3.0/24\" ] public_subnets = [ \"10.0.101.0/24\" , \"10.0.102.0/24\" , \"10.0.103.0/24\" ] assign_generated_ipv 6 _cidr_block = true enable_nat_gateway = true single_nat_gateway = true enable_dns_hostnames = true enable_dns_support = true public_subnet_tags = { \"kubernetes.io/cluster/eks-${local.prefix}-${local.env}\" = \"shared\" \"kubernetes.io/role/elb\" = \"1\" } private_subnet_tags = { \"kubernetes.io/cluster/eks-${local.prefix}-${local.env}\" = \"shared\" \"kubernetes.io/role/internal-elb\" = \"1\" } }","title":"VPC module"}]}